{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebd6bc6",
   "metadata": {},
   "source": [
    "Нейроны - биологическая основа всей разумной жизни на Земле, и казалось, бы при чем здесь математика? При том, что как выяснилось еще в 50е годы, модель нейрона математически очень проста:\n",
    "\n",
    "<img src=https://neerc.ifmo.ru/wiki/images/a/a5/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD_%D1%81%D1%85%D0%B5%D0%BC%D0%B0.png>\n",
    "\n",
    "Нейрон состоит из определенного числа “входов” (синапсов), каждый из которых имеет определенный весовой коэффициент Wij, собственно ячейки нейрона, суммирующей значения “входов”, некоторой функции активации (упрощенно говоря, нейрон переходит в “активное” состояние, если сумма больше некоторого порога), и выхода (аксона), который в свою очередь, может передать сигнал на следующий нейрон.\n",
    "Можно написать, что:\n",
    "$$ Sum = w_{11}*x_1 + w_{12}*x_2 + ... + w_{1n}*x_n = \\sum_{i=1}^{n} w_{1i}\\cdot x_i $$\n",
    "$$Out = \\phi(Sum)$$\n",
    "\n",
    "В качестве функции активации f часто используется *сигмоида* (хотя бывают и другие [варианты](https://ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD):\n",
    "\n",
    "$$\\sigma = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Как можно видеть, все весьма просто - математически нейрон лишь чуть сложнее чем обыкновенный сумматор. Выходы нейрона благодаря функции активации лежат в диапазоне 0..1.\n",
    "Что может делать один нейрон? Практически ничего полезного, но если объединить их в сеть, возможности возрастают кардинально. Например, сеть из 6 нейронов уже способна воспроизвести любую логическую функцию, что мы сейчас и продемонстрируем."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df7942",
   "metadata": {},
   "source": [
    "### многослойный перцептрон\n",
    "Рассмотрим структуру сети, называемую \"многослойный перцептрон\" (multilayer perceptron):\n",
    "<img src=https://matthewmazur.files.wordpress.com/2018/03/neural_network-7.png>\n",
    "\n",
    "Как можно видеть, она состоит из входного слоя нейронов (input layer), скрытого слоя (hidden layer) и выходного слоя (output layer). Каждый нейрон скрытого слоя соединен со всеми входными нейронами, плюс введено дополнительное “смещение” (bias), которое можно представить как нейрон, вход которого всегда = 1 и вес равен константе (например 0.5). Дальше структура повторяется. Количество входных, выходных и скрытых нейронов может быть различно.\n",
    "Процесс перехода сигнала по сети называется прямым распространением (forward propagation). Допустим, на входе имеются значения: x1 = 0, x2 = 1. Тогда прямое распространение выглядит так:\n",
    "\n",
    "Шаг-1: передача сигнала от входа во внутренний (hidden) слой\n",
    "\n",
    "  h1 = f(x1*w1 + x2*w2 + 0.5*1)\n",
    "  \n",
    "  h2 = f(x1*w3 + x2*w4 + 0.5*1)\n",
    "  \n",
    "Шаг-2: передача сигнала от внутреннего в наружный (output) слой\n",
    "\n",
    "  o1 = f(h1*w5 + h2*w6 + 0.5*1)\n",
    "  \n",
    "  o2 = f(h1*w7 + h2*w8 + 0.5*1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96cfa2",
   "metadata": {},
   "source": [
    "### Обучение \n",
    "Собственно и все, мы подали на вход сети сигнал “х” и получили на выходе сигнал “о”. Однако внимательный читатель может задать вопрос - а как задаются коэффициенты w_i, i=1,...,n?\n",
    "\n",
    "Это собственно, самое интересное и сложное, процесс настройки\n",
    "параметров сети и называется **обучением**. Задавая различные наборы входных и выходных данных, мы можем обучить сеть воспроизводить требуемые нам функции.\n",
    "Для данного типа сетей используется так называемый алгоритм **обратного\n",
    "распространения** (back propagation). Его суть в модификации коэффициентов w от конца сети к началу, примерно это можно выразить так:\n",
    "\n",
    "Шаг-0: Значение коэффициентов w1..w8 заполняются некоторыми случайными\n",
    "величинами.\n",
    "\n",
    "Шаг-1: Берем набор входных данных, получаем значения выхода (output) методом уже рассмотренного “прямого распространения”.\n",
    "\n",
    "Шаг-2: Сравниваем значения output с “целевыми” значениями (target). Для удобства сравнения часто используется функция квадрата разности: $Err = (output - target)^2$\n",
    "\n",
    "Шаг-3: Модифицируем “выходные” коэффициенты w5..w8 так, чтобы значение Err уменьшалось. Для этого используется метод так называемого “градиентного спуска”:\n",
    "функцию output всей нейронной сети в целом легко можно выразить как\n",
    "последовательность сумм и произведений (x1*w1 + x2*w2 ...), это значит, что продифференцировав ее по нужному весовому параметру, легко найти производную, например $\\frac{dOutput}{dw5}$. Как известно, производная показывает направление роста функции, что нам и нужно - мы можем изменить параметр w5 на величину, обратную производной.\n",
    "\n",
    "Шаг-4: Выполняем аналогичный процесс для “входных” коэффициентов w1..w4, формулы тут примерно аналогичны предыдущему пункту.\n",
    "Шаги 1..4 повторяются до тех пор, пока ошибка не станет меньше некоторой величины.\n",
    "Этот процесс весьма медленный, и может требоваться несколько тысяч итераций, однако выполняется он лишь однократно. Далее коэффициенты можно сохранить, и сеть уже обучена воспроизводить указанный при обучении набор данных. Эти весовые коэффициенты фактически и являются аналогом нейронных связей, появляющихся после обучения в “биологической” сети.\n",
    "Более подробно формулы с примерами вычислений можно найти [здесь](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/). А мы\n",
    "рассмотрим программу на языке Python, реализующую данный метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "842b7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,n_in,n_hidden, n_out):\n",
    "        \"\"\"\n",
    "        Класс MLP, принимающий на входе 3 параметра: \n",
    "        число входных нейронов, \n",
    "        число скрытых нейронов, \n",
    "        число выходных нейронов\n",
    "        \"\"\"\n",
    "        self.n_in = n_in # число нейронов во входном слое \n",
    "        self.n_hidden = n_hidden # число нейронов в скрытом слое\n",
    "        self.n_out = n_out # число нейронов в выходном слое \n",
    "        # Входные данные: массив [n_in + 1] инициализирован значениями 1.0\n",
    "        self.inputLayer = [1.0]*(n_in + 1)\n",
    "        # Веса входного-скрытого слоя: матрица [n_in+1]x[n_hidden] заполнена\n",
    "        # случайеыми числами от 0 до 1\n",
    "        self.wIH = [[random.random() for x in range(n_in + 1)] for y in range(n_hidden)]\n",
    "        # Скрытые нейроны: [n_hidden + 1]\n",
    "        self.hiddenLayer = [1.0]*(n_hidden + 1)\n",
    "        # Веса нейронов скрытого-выходного слоя: матрица [n_hidden+1]x[n_out] заполнена\n",
    "        # случайными числами от 0 до 1\n",
    "        self.wHO = [[random.random() for x in range(n_hidden+1)] for y in range(n_out)]\n",
    "        # Выходной слой: массив\n",
    "        self.outputLayer = [1.0]*n_out\n",
    "        \n",
    "    def printValues(self):\n",
    "        print (\"Network: input/hidden/output: {}/{}/{}\".format(self.n_in, self.n_hidden, self.n_out))\n",
    "        print (\"Вход\", self.inputLayer)\n",
    "        print (\"wIH\", self.wIH)\n",
    "        print (\"Скрытый слой\", self.hiddenLayer)\n",
    "        print (\"wHO\", self.wHO)\n",
    "        print (\"Выход\", self.outputLayer)\n",
    "        \n",
    "    def printOutput(self):\n",
    "        print (\"Вход\", self.inputLayer)\n",
    "        print (\"Выход\", self.outputLayer)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\" Функция активации (сигмоида)\"\"\"\n",
    "        return 1.0/(1.0 + math.exp(-x))\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        \"\"\"Производная функции активации\"\"\"\n",
    "        return x*(1 - x)\n",
    "    \n",
    "    def forwardPass(self, x):\n",
    "        # Прямое распространение сигнала\n",
    "        # 1.Input\n",
    "        for p in range(len(x)):\n",
    "            self.inputLayer[p] = x[p]\n",
    "            \n",
    "        # 2.Input-Hidden\n",
    "        for h in range(self.n_hidden):\n",
    "            sum = 0.0\n",
    "            for i in range(len(self.inputLayer)):\n",
    "                sum += self.inputLayer[i]*self.wIH[h][i]\n",
    "            self.hiddenLayer[h] = self.sigmoid(sum)\n",
    "        \n",
    "        # 3.Hidden-Output\n",
    "        for o in range(self.n_out):\n",
    "            sum = 0.0\n",
    "            for h in range(len(self.hiddenLayer)):\n",
    "                sum += self.hiddenLayer[h]*self.wHO[o][h]\n",
    "            self.outputLayer[o] = self.sigmoid(sum)\n",
    "            \n",
    "    def backPass(self, input, target):\n",
    "        \"\"\"Обратное распространение\n",
    "           Формулы взяты из статьи\n",
    "           https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
    "        \"\"\"\n",
    "        learn_rate = 0.5\n",
    "        # Output->hidden layer adjust weights\n",
    "        for o in range(self.n_out):\n",
    "            for h in range(self.n_hidden):\n",
    "                deout = -(target[o] - self.outputLayer[o])\n",
    "                der = self.derivative(self.outputLayer[o])\n",
    "                hid = self.hiddenLayer[h]\n",
    "                der = deout*der*hid\n",
    "                prev = self.wHO[o][h]\n",
    "                self.wHO[o][h] -= learn_rate*der\n",
    "\n",
    "        # Hidden->input layer adjust weights\n",
    "        for h in range(self.n_hidden):\n",
    "            for i in range(self.n_in):\n",
    "                derSum = 0.0\n",
    "                for o in range(self.n_out):\n",
    "                    deout = -(target[o] - self.outputLayer[o])\n",
    "                    derOut = self.derivative(self.outputLayer[o])\n",
    "                    der = deout*derOut*self.wHO[o][h]\n",
    "                    derSum += der\n",
    "                    \n",
    "                derH = self.derivative(self.hiddenLayer[h])\n",
    "                derI = self.inputLayer[i]\n",
    "                der = derSum*derH*derI\n",
    "                prev = self.wIH[h][i]\n",
    "                self.wIH[h][i] -= learn_rate*der\n",
    "                \n",
    "    def calcError(self, x, target):\n",
    "        \"\"\"Вычисление ошибки: 0.5*Sum(dV^2)\"\"\"\n",
    "        err = 0.0\n",
    "        for p in range(self.n_out):\n",
    "            err += 0.5*((self.outputLayer[p] - target[p])**2)\n",
    "        return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "721e92e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тренировка сети\n",
      "Step 0 error 0.3637716556170986\n",
      "Step 10000 error 0.002591567414418018\n",
      "Step 20000 error 0.0001464041930804557\n",
      "Step 30000 error 0.0006465036512996516\n",
      "Step 40000 error 0.0004493848175402304\n",
      "Step 50000 error 0.0003599132129924134\n",
      "Step 60000 error 2.4689899312746475e-05\n",
      "Step 70000 error 0.00023247666395566355\n",
      "Step 80000 error 1.6897813249508125e-05\n",
      "Step 90000 error 0.00018853271519806775\n",
      "Step 100000 error 0.00016672341527350655\n",
      "Step 110000 error 0.00015281793936023082\n",
      "Step 120000 error 0.0003351033375027316\n",
      "Step 130000 error 0.0001290239992486286\n",
      "Step 140000 error 0.00027751709069232764\n",
      "Step 150000 error 6.933971334374419e-06\n",
      "Step 160000 error 0.0002512229151484009\n",
      "Step 170000 error 0.00023319197932234877\n",
      "Step 180000 error 9.236945865748288e-05\n",
      "Step 190000 error 0.00020989169472304737\n",
      "Step 200000 error 8.028666242710179e-05\n",
      "Step 210000 error 7.802940731997645e-05\n",
      "Step 220000 error 7.584822850103807e-05\n",
      "Step 230000 error 7.190433447220447e-05\n",
      "Step 240000 error 6.610980013762894e-05\n",
      "Step 250000 error 0.00015954082825882983\n",
      "Step 260000 error 6.177213537001375e-05\n",
      "Step 270000 error 5.928707270741622e-05\n",
      "Step 280000 error 3.0913220671492316e-06\n",
      "Step 290000 error 5.5430088041191995e-05\n",
      "Step 300000 error 5.407840447372227e-05\n",
      "Step 310000 error 2.716322605006641e-06\n",
      "Step 320000 error 5.061567891696008e-05\n",
      "Step 330000 error 2.4378835309498028e-06\n",
      "Step 340000 error 4.7804742821848206e-05\n",
      "Step 350000 error 4.681545207545918e-05\n",
      "Step 360000 error 0.00010820601535832559\n",
      "Step 370000 error 2.172888353952713e-06\n",
      "Step 380000 error 4.2611986233918014e-05\n",
      "Step 390000 error 0.00010114529209304328\n",
      "Step 400000 error 4.0239088809426256e-05\n",
      "Step 410000 error 3.926623611857062e-05\n",
      "Step 420000 error 1.825162850221473e-06\n",
      "Step 430000 error 3.7736939519069835e-05\n",
      "Step 440000 error 3.6670227429160295e-05\n",
      "Step 450000 error 3.566582124428298e-05\n",
      "Step 460000 error 3.4411152238580235e-05\n",
      "Step 470000 error 3.412179347456072e-05\n",
      "Step 480000 error 3.31064441084963e-05\n",
      "Step 490000 error 1.5115158356418066e-06\n",
      "\n",
      "Done\n",
      "Network: input/hidden/output: 2/3/1\n",
      "Вход [0.0, 1.0, 1.0]\n",
      "wIH [[-5.008211266598422, 6.951498389138338, 0.23648393331145523], [5.55115600174293, 5.5301220399020155, 0.38335300484986456], [6.985500238700786, -5.027801624010026, 0.24878686757502666]]\n",
      "Скрытый слой [0.9992449586079535, 0.9973045027536134, 0.008334262529680423, 1.0]\n",
      "wHO [[-14.18302110281847, 18.84455754035811, -14.194142085825902, 0.32077506931652733]]\n",
      "Выход [0.9920283824433579]\n",
      "\n",
      "Результаты\n",
      "Вход [0.0, 0.0, 1.0]\n",
      "Выход [0.012441139914418786]\n",
      "Вход [0.0, 1.0, 1.0]\n",
      "Выход [0.9920288833511246]\n",
      "Вход [1.0, 0.0, 1.0]\n",
      "Выход [0.9919389706399067]\n",
      "Вход [1.0, 1.0, 1.0]\n",
      "Выход [0.0017200784023550875]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(n_in=2, n_hidden=3, n_out=1)\n",
    "\n",
    "print(\"Тренировка сети\")\n",
    "inputs = [ [0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0] ]\n",
    "targets = [ [0.0], [1.0], [1.0], [0.0] ]\n",
    "N = 500000\n",
    "for p in range(N):\n",
    "    rand_index = random.randrange(0, len(inputs))\n",
    "    mlp.forwardPass(inputs[rand_index])\n",
    "    mlp.backPass(inputs[rand_index], targets[rand_index])\n",
    "    err = mlp.calcError(inputs[rand_index], targets[rand_index])\n",
    "    if p % 10000 == 0:\n",
    "        print(\"Step\", p, \"error\", err)\n",
    "print(\"\")\n",
    "print(\"Done\")\n",
    "mlp.printValues()\n",
    "print(\"\")\n",
    "print(\"Результаты\")\n",
    "mlp.forwardPass([ 0.0, 0.0 ])\n",
    "mlp.printOutput()\n",
    "mlp.forwardPass([ 0.0, 1.0 ])\n",
    "mlp.printOutput()\n",
    "mlp.forwardPass([ 1.0, 0.0 ])\n",
    "mlp.printOutput()\n",
    "mlp.forwardPass([ 1.0, 1.0 ])\n",
    "mlp.printOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c5e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вход [0.0, 0.0, 1.0]\n",
      "Выход [0.012441139914418786]\n",
      "Вход [0.0, 1.0, 1.0]\n",
      "Выход [0.9920288833511246]\n",
      "Вход [1.0, 0.0, 1.0]\n",
      "Выход [0.9919389706399067]\n",
      "Вход [1.0, 1.0, 1.0]\n",
      "Выход [0.0017200784023550875]\n"
     ]
    }
   ],
   "source": [
    "mlp.forwardPass([ 0.0, 0.0 ])\n",
    "mlp.printOutput()\n",
    "mlp.forwardPass([ 0.0, 1.0 ])\n",
    "mlp.printOutput()\n",
    "mlp.forwardPass([ 1.0, 0.0 ])\n",
    "mlp.printOutput()\n",
    "mlp.forwardPass([ 1.0, 1.0 ])\n",
    "mlp.printOutput()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df1adc4d",
   "metadata": {},
   "source": [
    "### Как MLP распознаtn рукописные цифры\n",
    "Рассмотрим практический пример - распознавание выше написанной нейронной сетью рукописного текста. Для тренировки сети воспользуемся имеющейся в открытом доступе базой MNIST, содержащей 60000 черно-белых рукописных изображений цифр размером 28x28пкс.\n",
    "\n",
    "- цифры - \"C:\\Users\\user\\BigData Developer 2018\\lecture11\\mnist2500_labels.txt\"\n",
    "- сканы цифр - \"C:\\Users\\user\\BigData Developer 2018\\lecture11\\mnist2500_X.txt\"\n",
    "\n",
    "Выглядят изображения примерно так:\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png>\n",
    "Массив изображений каждой цифры хранится в виде чисел float в диапазоне от 0 до 1, длина массива равна 28х28 = 784 значения. Кстати, при выводе массива в консоль форма цифры вполне узнаваема:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4573030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "1111111111111111111111111111  |  ############################\n",
      "1111111111111111111111111111  |  ############################\n",
      "1111111111111111111111111111  |  ############################\n",
      "1111111111111111111111111111  |  ############################\n",
      "1111111111111111111111111111  |  ############################\n",
      "1110000000000000111111111111  |  ###             ############\n",
      "1110000000000000000111111111  |  ###                #########\n",
      "1110000000000000000111111111  |  ###                #########\n",
      "1110000011111100000111111111  |  ###     ######     #########\n",
      "1111111111111100000111111111  |  ##############     #########\n",
      "1111111111110000001111111111  |  ############      ##########\n",
      "1111111111100000001111111111  |  ###########       ##########\n",
      "1111111111000000001111111111  |  ##########        ##########\n",
      "1111111111000000000011111111  |  ##########          ########\n",
      "1111111111000000000000111111  |  ##########            ######\n",
      "1111111111110000000000011111  |  ############           #####\n",
      "1111111111111111000000011111  |  ################       #####\n",
      "1111111111111111100000011111  |  #################      #####\n",
      "1111111111111111110000011111  |  ##################     #####\n",
      "1111111100011111100000011111  |  ########   ######      #####\n",
      "1111111000011111000000111111  |  #######    #####      ######\n",
      "1111111000001100000000111111  |  #######     ##        ######\n",
      "1111111000000000000001111111  |  #######              #######\n",
      "1111111000000000000011111111  |  #######             ########\n",
      "1111111100000000001111111111  |  ########          ##########\n",
      "1111111111111111111111111111  |  ############################\n",
      "1111111111111111111111111111  |  ############################\n",
      "1111111111111111111111111111  |  ############################\n"
     ]
    }
   ],
   "source": [
    "n=12\n",
    "print(y[n]) #какой цифре этот массив соответствует\n",
    "for i in np.reshape(X[n], (28, 28)).T: # список длины 784 превратили в матрицу 28х28 и транспонировали:    \n",
    "    lst = map(str,list(map(int, i))) # поэлементно округлили до целого и сконвертили в строку\n",
    "    lst1 = map(lambda x: '#' if x!=0  else ' ',list(map(int, i))) # нули заменяем пробелом, а 1 -> #\n",
    "    print(\"\".join(lst),' | ',\"\".join(lst1)) #элементы списка объединили в строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6903a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.loadtxt(\"BigData Developer 2018\\lecture11\\mnist2500_X.txt\")\n",
    "y = np.loadtxt(\"BigData Developer 2018\\lecture11\\mnist2500_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410cefd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2500, 784), (2500,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82dc1fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Подготовка данных\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(n_in=28*28, n_hidden=48, n_out=10)\n",
    "\n",
    "print (\"2. Подготовка данных\")\n",
    "batch = 50\n",
    "inputs = []\n",
    "targets = []\n",
    "for p in range(batch):\n",
    "    data = list(X[p])\n",
    "    resulVal = int(y[p])\n",
    "    # Конвертация целевого числа в массив: “3” => [0,0,0,1,0,0,0,0,0,0]\n",
    "    result_flat = [0.0]*10\n",
    "    result_flat[resulVal] = 1.0\n",
    "    inputs.append(data)\n",
    "    targets.append(result_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2997194a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b3b56920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Обучение\n",
      "Step 0 error 224.99999995337785\n"
     ]
    }
   ],
   "source": [
    "# print (\"3. Обучение\")\n",
    "\n",
    "# try:\n",
    "#     N = 10000\n",
    "#     for p in range(N):\n",
    "#         errSum = 0.0\n",
    "#         for d in range(batch):\n",
    "#             mlp.forwardPass(inputs[d])\n",
    "#             mlp.backPass(inputs[d], targets[d])\n",
    "#             errSum += mlp.calcError(inputs[d], targets[d])\n",
    "#         print (\"Step\", p, \"error\", errSum)\n",
    "# except KeyboardInterrupt:\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd76ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

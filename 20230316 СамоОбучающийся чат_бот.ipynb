{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dbe2856",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 100>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# #         n=2\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# #         while (X.shape[0]//n > n) and (n<=5):\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# #             n+=1\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m#     clf = clfs[ind_best]\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m#     print(clf)    \u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_generative_replica\u001b[39m(text): \u001b[38;5;66;03m#генерим ответ на запрос\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     text_vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([text])\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# векторизируем запрос\u001b[39;00m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#clfs = [LogisticRegression(),  GradientBoostingClassifier(),  RandomForestClassifier(), \u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# BaggingClassifier(), ExtraTreesClassifier(),  AdaBoostClassifier()]\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#for clf in clfs:\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#clf = LogisticRegression()\u001b[39;00m\n\u001b[0;32m     85\u001b[0m clf \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier()    \n\u001b[1;32m---> 86\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:745\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    738\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    739\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    740\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    741\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    742\u001b[0m     )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:247\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    244\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    246\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 247\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    250\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    251\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    252\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    259\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    260\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \n\u001b[0;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:430\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_ \u001b[38;5;241m=\u001b[39m Tree(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_ \u001b[38;5;241m=\u001b[39m \u001b[43mTree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features_in_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# TODO: tree shouldn't need this in this case\u001b[39;49;00m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;66;03m# Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_leaf_nodes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:640\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telebot, wikipedia, re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import datetime\n",
    "\n",
    "\n",
    "bot = telebot.TeleBot('2130329456:AAGRRUymCdLRgfm40dOBgOIyNJdwzDjRN10')\n",
    "\n",
    "wikipedia.set_lang(\"ru\")\n",
    "\n",
    "def greeting():\n",
    "    \"\"\"\n",
    "    Функция приветствия в зависимости от времени суток\n",
    "    \"\"\"\n",
    "    opts = { \"hey\" : ('Привет! ', 'Здравствуйте! ', 'Доброе утро! ', 'Добрый день! ', 'Добрый вечер! ', 'Доброй ночи! ')}\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    #now += timedelta(hours = 3) # можешь проверить тут\n",
    "    #print(now)\n",
    "    if now.hour > 4 and now.hour <= 12 :\n",
    "        greet = opts[\"hey\"][2]\n",
    "    if now.hour > 12 and now.hour <= 16 :\n",
    "        greet = opts[\"hey\"][3]\n",
    "    if now.hour > 16 and now.hour <= 24 :\n",
    "        greet = opts[\"hey\"][4]\n",
    "    if now.hour >= 0 and now.hour <= 4 :\n",
    "        greet = opts[\"hey\"][5]\n",
    "    return greet\n",
    "\n",
    "\n",
    "\n",
    "alphabet = ' 1234567890-йцукенгшщзхъфывапролджэячсмитьбюёqwertyuiopasdfghjklzxcvbnm?%.,()!:;'\n",
    "\n",
    "def clean_str(r):\n",
    "    r = r.lower()\n",
    "    r = [c for c in r if c in alphabet]\n",
    "    return ''.join(r)\n",
    "\n",
    "def update():\n",
    "    with open('chat_bot_BD.txt', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    blocks = content.split('\\n')\n",
    "    dataset = []\n",
    "    \n",
    "    for block in blocks:\n",
    "        replicas = block.split('\\\\')[:2]\n",
    "        if len(replicas) == 2:\n",
    "            pair = [clean_str(replicas[0]), clean_str(replicas[1])]\n",
    "            if pair[0] and pair[1]:\n",
    "                dataset.append(pair)\n",
    "#      как выглядит база\n",
    "#     [question, answer]\n",
    "#     [['привет', 'здравствуйте!'],\n",
    "#      ['как дела', 'хорошо.']]\n",
    "    \n",
    "    \n",
    "    # создаём два списка\n",
    "    X_text = []\n",
    "    y = []\n",
    "    \n",
    "    for question, answer in dataset[:10000]:\n",
    "        X_text.append(question) # список вопросов\n",
    "        y += [answer] # список ответов\n",
    "    \n",
    "    \n",
    "    # машинное обучение \n",
    "    global vectorizer\n",
    "    vectorizer = CountVectorizer() # CountVectorizer по умолчанию  производит токенизацию и выкидывает слова с длиной меньшей чем два.\n",
    "    X = vectorizer.fit_transform(X_text) #\n",
    "    \n",
    "    #scores_list = []\n",
    "    global clf\n",
    "    #clfs = [LogisticRegression(),  GradientBoostingClassifier(),  RandomForestClassifier(), \n",
    "    # BaggingClassifier(), ExtraTreesClassifier(),  AdaBoostClassifier()]\n",
    "    #for clf in clfs:\n",
    "    #clf = LogisticRegression()\n",
    "    clf = GradientBoostingClassifier()    \n",
    "    clf.fit(X, y)\n",
    "# #         n=2\n",
    "# #         while (X.shape[0]//n > n) and (n<=5):\n",
    "# #             n+=1\n",
    "# #         try: \n",
    "# #             scores = cross_val_score(clf, X, y, cv=n-2)\n",
    "# #             scores_list.append(scores.mean())\n",
    "# #         except:\n",
    "#         scores = cross_val_score(clf, X, y, cv=3)\n",
    "#         scores_list.append(scores.mean())\n",
    "#         ind_best = scores_list.index(max(scores_list))\n",
    "#     clf = clfs[ind_best]\n",
    "#     print(clf)    \n",
    "\n",
    "update()\n",
    "\n",
    "def get_generative_replica(text): #генерим ответ на запрос\n",
    "    text_vector = vectorizer.transform([text]).toarray()[0] # векторизируем запрос\n",
    "    question = clf.predict([text_vector])[0] #по вектору вычисляем ближайший вопрос\n",
    "    return question\n",
    "\n",
    "def getwiki(s):\n",
    "    try:\n",
    "        ny = wikipedia.page(s)\n",
    "        wikitext=ny.content[:1000]\n",
    "        wikimas=wikitext.split('.')\n",
    "        wikimas = wikimas[:-1]\n",
    "        wikitext2 = ''\n",
    "        for x in wikimas:\n",
    "            if not('==' in x):\n",
    "                if(len((x.strip()))>3):\n",
    "                    wikitext2=wikitext2+x+'.'\n",
    "            else:\n",
    "                break\n",
    "        wikitext2=re.sub('\\([^()]*\\)', '', wikitext2)\n",
    "        wikitext2=re.sub('\\([^()]*\\)', '', wikitext2)\n",
    "        wikitext2=re.sub('\\{[^\\{\\}]*\\}', '', wikitext2)\n",
    "        return wikitext2\n",
    "    except Exception as e:\n",
    "        return 'В энциклопедии нет информации об этом'\n",
    "\n",
    "@bot.message_handler(commands=['start'])\n",
    "def start_message(message):\n",
    "    bot.send_message(message.chat.id,\"Здравствуйте, Сэр.\")\n",
    "\n",
    "question = \"\"\n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def get_text_messages(message):\n",
    "    command = message.text.lower()\n",
    "    if command in ('хай','хей','хаюшки','здрасьте', 'салам','привет','приветик', 'здорово','здрасьте','здравствуйте', \\\n",
    "                'доброе утро', 'добрый день', 'добрый вечер', 'доброй ночи','приветствую','здрасте',\\\n",
    "               'салют', 'мое почтение', 'наше вам', 'здоровеньки','здравия желаю', 'чао', 'хелло', 'физкульт', 'шалом'):\n",
    "        bot.send_message(message.from_user.id, greeting()) \n",
    "    elif command ==\"--\":\n",
    "        bot.send_message(message.from_user.id, \"а как правильно?\")\n",
    "        bot.register_next_step_handler(message, wrong)\n",
    "    else:\n",
    "        global question\n",
    "        question = command\n",
    "        reply = get_generative_replica(command)\n",
    "        print(reply)        \n",
    "        if \"вики\" in reply:\n",
    "            bot.send_message(message.from_user.id, getwiki(command))\n",
    "        else:\n",
    "            bot.send_message(message.from_user.id, reply)\n",
    "\n",
    "def wrong(message): # функция записывает правильный ответ в файл\n",
    "    a = \"{}\\{}\\n\".format(question, message.text.lower())\n",
    "        #{question}\\{message.text.lower()}.\\n\"\n",
    "    with open('chat_bot_BD.txt', \"a\", encoding='utf-8') as f:\n",
    "        f.write(a)\n",
    "    bot.send_message(message.from_user.id, \"Готово\")\n",
    "    update()\n",
    "\n",
    "bot.polling(none_stop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323b9ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получим список возможных параметров выбранной модели clf = GradientBoostingClassifier()    \n",
    "clf.get_params()#.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0fc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chat_bot_BD.txt', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "blocks = content.split('\\n')\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b57f0141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет\\\\здравствуйте!',\n",
       " 'как дела\\\\хорошо...',\n",
       " 'кто ты\\\\я чат-бот',\n",
       " 'ты знаешь колю\\\\да ',\n",
       " 'кошка\\\\маленькое пушистое животное, которое мяукает',\n",
       " 'кот\\\\кошка мужского пола',\n",
       " 'жмот\\\\жадный человек',\n",
       " 'что нового\\\\новостей нет',\n",
       " 'что ты можешь\\\\я могу отвечать на ваши сообщения',\n",
       " 'круто\\\\а вы думали!',\n",
       " 'поговорим\\\\конечно',\n",
       " 'драндулет\\\\ расдолбанный, старый велосипед',\n",
       " 'кратер\\\\Отверстие в вершине вулкана, через которое извергается лава.',\n",
       " 'геродот\\\\вики',\n",
       " 'интеграл\\\\одно из важнейших понятий математического анализа, которое возникает при решении задач: о нахождении площади под кривой; пройденного пути при неравномерном движении; массы неоднородного тела, и тому подобных; а также в задаче о восстановлении функции по её производной',\n",
       " 'пижон\\\\человек у которого количество жен равно числу пи = 3.14',\n",
       " 'пеньюар\\\\дурак из южной африки',\n",
       " 'Автомат\\\\Самокритика',\n",
       " 'Алимент\\\\Гастарбайтер, работающий в полиции',\n",
       " 'Армани\\\\ Военный бюджет; Армянские деньги; Рука Ани',\n",
       " 'Бабочки\\\\Женские очки',\n",
       " 'Бабуин\\\\Пригласите даму войти',\n",
       " 'Байкал\\\\Спуск воды в унитазе',\n",
       " 'Бакшиш\\\\Бензина нет (объявление на бензоколонке)',\n",
       " 'Бармалей\\\\Расходившаяся в баре особа',\n",
       " 'Безмен\\\\Лесбийская любовь; Девичник',\n",
       " 'Благородный\\\\Влиятельный родственник',\n",
       " 'Блокада\\\\Квартира в блочном доме',\n",
       " 'Блюститель\\\\Обольститель нетрадиционной ориентации',\n",
       " 'Брошка\\\\Женщина, которую бросил муж',\n",
       " 'Букашка\\\\Мягко говоря, бывшая в употреблении каша',\n",
       " 'Бульдозер\\\\Человек, мастерски разливающий на троих (по булькам)',\n",
       " 'Буржуй\\\\Традиционное наказание у нефтяников за не там пробуренную скважину',\n",
       " 'Вокал\\\\Отличный стул! (медицинский термин)',\n",
       " 'Гегель\\\\Гоголь (на гейском языке)',\n",
       " 'Гейша\\\\Запрет на гей-парады',\n",
       " 'Гламур\\\\Штаб-квартира Московского уголовного розыска',\n",
       " 'Дворник\\\\Дворовое прозвище (часто радикально отличается от Интернет-ника, который человек сам себе придумывает)',\n",
       " 'Двуликий анус\\\\Гей-изменщик',\n",
       " 'Дездемона\\\\Очень плохое ТСЖ',\n",
       " 'Дрянь\\\\День рождения мужчины',\n",
       " 'Заманить\\\\Дать взятку ',\n",
       " 'Зря зри\\\\Смотри очень внимательно, но при этом делай вид, что ничего не видишь',\n",
       " 'Индюк\\\\Высокий иностранный гость',\n",
       " 'Картуз\\\\Очень хороший автомобиль',\n",
       " 'Киска Форте\\\\Поцелуй-ка меня крепко-крепко',\n",
       " 'Консервы\\\\Консерваторы',\n",
       " 'Лавина\\\\Большая любовь',\n",
       " 'Лавка\\\\Небольшая мимолетная любовь (максимум курортный роман)',\n",
       " 'Лумумба\\\\Лучше умный самоучка, чем дурень с МБА',\n",
       " 'Манипулятор\\\\Человек, разбрасывающийся деньгами',\n",
       " 'Матфей\\\\Прежде было имя, теперь – словосочетание',\n",
       " 'Манок\\\\Неплохой человек',\n",
       " 'Мелитополь\\\\Я спросил у тополя',\n",
       " 'Миллион\\\\Сомнение невесты, которое сразу рассеивается, если это же слово произносит жених',\n",
       " 'Мобильная связь\\\\Связь, готовая приехать по вызову в любой момент и в любое место.',\n",
       " 'Морфей\\\\Естественное продолжение \"Охоты на ведьм\".',\n",
       " 'Мудрость\\\\Краткий конспект последней строчки стишка \"Восьмое марта близко-близко\".',\n",
       " 'Нановации\\\\ Слабенькие инновации; Жидкие аплодисменты',\n",
       " 'Непорочное финансирование',\n",
       " 'Неизвестный пока в России метод безоткатного бюджетного финансирования',\n",
       " 'Ниссан\\\\Новый матрац',\n",
       " 'Ноу-хау\\\\Нет! А как?',\n",
       " 'Огород\\\\Очень родовитый, породистый',\n",
       " 'Окрошка\\\\Радостное восклицание при появлении любимой (\"крошки\")',\n",
       " 'ОПГ\\\\Организованная полицейская группировка или Организованная правительственная группировка',\n",
       " 'Отрада\\\\Украинские законы',\n",
       " 'Парад\\\\Ну, очень плохой пар!',\n",
       " 'Персиянка\\\\Дама с обворожительными персями',\n",
       " 'Полпред\\\\Наполовину преданный',\n",
       " 'Полынь-Поляна\\\\Наполовину женщина, наполовину мужчина',\n",
       " 'Понедельник\\\\Ничего не делающий человек, которому всё по ...',\n",
       " 'Поститься\\\\Активно писать посты в социальных сетях',\n",
       " 'Романтик\\\\Ром очень хорошей выдержки',\n",
       " 'Самолюбие\\\\Разновидность безопасного секса',\n",
       " 'Сварка\\\\Спевка овощей, готовящихся превратиться в оливье',\n",
       " 'Сделать вывод\\\\Завершить акт любви',\n",
       " 'Сергей\\\\Почтительное обращение к человеку нетрадиционной сексуальной ориентации',\n",
       " 'Система\\\\Тема разговора о женских прелестях',\n",
       " 'Скелет в шкафу\\\\Забытый любовник',\n",
       " 'Снегурочка\\\\Членка преступной группировки с холодным характером.',\n",
       " 'Союзник\\\\Человек, с которым совместно пользуешь что-то или кого-то',\n",
       " 'Социальная несправедливость\\\\Корпоративные правила, запрещающие сотрудникам выходить в социальные сети.',\n",
       " 'Спать\\\\Посещать спа-салон',\n",
       " 'Спина\\\\Спокойной ночи, дорогая\\\\дорогой; Колыбельная',\n",
       " 'Столичный\\\\Человек, не ограничившийся двуличностью',\n",
       " 'Стоматология\\\\Наука о том, что можно делать с помидорами',\n",
       " 'Сторож\\\\Заседание правительства',\n",
       " 'Сукно\\\\Вход с собаками воспрещен!',\n",
       " 'Телеграф\\\\Граф, уехавший в командировку',\n",
       " 'Трахея\\\\Он ее полюбил (древнерус.)',\n",
       " 'Трибунал\\\\Коррумпированный суд, требующий наличных от подсудимого.',\n",
       " 'Ухажёр\\\\Страстный поцелуй в ухо',\n",
       " 'Факероносцы\\\\Население диктаторской страны',\n",
       " 'Факты\\\\Да пошел ты!',\n",
       " 'Фридом\\\\Бесплатный дом – уникальная привилегия слуг народа (от англ. \"freedom\")',\n",
       " 'Херсон\\\\Бессонница',\n",
       " 'Хулиган\\\\Да что мне твой пистолет?',\n",
       " 'Частокол\\\\Дальнейшее развитие темы картины \"Опять двойка\"',\n",
       " 'Чемпион\\\\Мужской детородный орган',\n",
       " 'Чернослив\\\\Сброс промышленных отходов',\n",
       " 'Экстерьер\\\\Терьер после операции по смене породы']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687d0149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет\\\\здравствуйте!',\n",
       " 'как дела\\\\хорошо...',\n",
       " 'кто ты\\\\я чат-бот',\n",
       " 'ты знаешь колю\\\\да ',\n",
       " 'кошка\\\\маленькое пушистое животное, которое мяукает',\n",
       " 'кот\\\\кошка мужского пола',\n",
       " 'жмот\\\\жадный человек',\n",
       " 'что нового\\\\новостей нет',\n",
       " 'что ты можешь\\\\я могу отвечать на ваши сообщения',\n",
       " 'круто\\\\а вы думали!',\n",
       " 'поговорим\\\\конечно',\n",
       " 'драндулет\\\\ расдолбанный, старый велосипед',\n",
       " 'кратер\\\\Отверстие в вершине вулкана, через которое извергается лава.',\n",
       " 'геродот\\\\вики',\n",
       " 'интеграл\\\\одно из важнейших понятий математического анализа, которое возникает при решении задач: о нахождении площади под кривой; пройденного пути при неравномерном движении; массы неоднородного тела, и тому подобных; а также в задаче о восстановлении функции по её производной',\n",
       " 'пижон\\\\человек у которого количество жен равно числу пи = 3.14',\n",
       " 'пеньюар\\\\дурак из южной африки',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8babc54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['привет', 'здравствуйте!']\n",
      "['как дела', 'хорошо.']\n",
      "['кто ты', 'я чат-бот.']\n",
      "['ты знаешь колю', 'да. ']\n",
      "['кошка', 'маленькое пушистое животное, которое мяукает']\n",
      "['кот', 'кошка мужского пола']\n",
      "['жмот', 'жадный человек']\n",
      "['что нового', 'новостей нет']\n",
      "['что ты можешь', 'я могу отвечать на ваши сообщения']\n",
      "['круто', 'а вы думали!']\n",
      "['поговорим', 'конечно']\n",
      "['вики', 'вики']\n",
      "['геродот', 'вики']\n",
      "['драндулет', 'вики']\n",
      "['драндулет', ' расдолбанный, старый велосипед']\n",
      "['кратер', 'Отверстие в вершине вулкана, через к-рое извергается лава.']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    replicas = block.split('\\\\')[:2]\n",
    "    print(replicas)\n",
    "    if len(replicas) == 2:\n",
    "        pair = [clean_str(replicas[0]), clean_str(replicas[1])]\n",
    "        if pair[0] and pair[1]:\n",
    "            dataset.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64877eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['привет', 'здравствуйте!'],\n",
       " ['как дела', 'хорошо.'],\n",
       " ['кто ты', 'я чат-бот.'],\n",
       " ['ты знаешь колю', 'да. '],\n",
       " ['кошка', 'маленькое пушистое животное, которое мяукает'],\n",
       " ['кот', 'кошка мужского пола'],\n",
       " ['жмот', 'жадный человек'],\n",
       " ['что нового', 'новостей нет'],\n",
       " ['что ты можешь', 'я могу отвечать на ваши сообщения'],\n",
       " ['круто', 'а вы думали!'],\n",
       " ['поговорим', 'конечно'],\n",
       " ['вики', 'вики'],\n",
       " ['геродот', 'вики'],\n",
       " ['драндулет', 'вики'],\n",
       " ['драндулет', ' расдолбанный, старый велосипед'],\n",
       " ['кратер', 'отверстие в вершине вулкана, через к-рое извергается лава.']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28e8abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "привет - здравствуйте!\n",
      "как дела - хорошо.\n",
      "кто ты - я чат-бот.\n",
      "ты знаешь колю - да. \n",
      "кошка - маленькое пушистое животное, которое мяукает\n",
      "кот - кошка мужского пола\n",
      "жмот - жадный человек\n",
      "что нового - новостей нет\n",
      "что ты можешь - я могу отвечать на ваши сообщения\n",
      "круто - а вы думали!\n",
      "поговорим - конечно\n",
      "вики - вики\n",
      "геродот - вики\n",
      "драндулет - вики\n",
      "драндулет -  расдолбанный, старый велосипед\n",
      "кратер - отверстие в вершине вулкана, через к-рое извергается лава.\n"
     ]
    }
   ],
   "source": [
    "X_text = []\n",
    "y = []\n",
    "for question, answer in dataset[:10000]:\n",
    "    print(question, '-',answer)\n",
    "    X_text.append(question)\n",
    "    y += [answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b76fc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет',\n",
       " 'как дела',\n",
       " 'кто ты',\n",
       " 'ты знаешь колю',\n",
       " 'кошка',\n",
       " 'кот',\n",
       " 'жмот',\n",
       " 'что нового',\n",
       " 'что ты можешь',\n",
       " 'круто',\n",
       " 'поговорим',\n",
       " 'вики',\n",
       " 'геродот',\n",
       " 'драндулет',\n",
       " 'драндулет',\n",
       " 'кратер']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "209687c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['здравствуйте!',\n",
       " 'хорошо.',\n",
       " 'я чат-бот.',\n",
       " 'да. ',\n",
       " 'маленькое пушистое животное, которое мяукает',\n",
       " 'кошка мужского пола',\n",
       " 'жадный человек',\n",
       " 'новостей нет',\n",
       " 'я могу отвечать на ваши сообщения',\n",
       " 'а вы думали!',\n",
       " 'конечно',\n",
       " 'вики',\n",
       " 'вики',\n",
       " 'вики',\n",
       " ' расдолбанный, старый велосипед',\n",
       " 'отверстие в вершине вулкана, через к-рое извергается лава.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1647b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer() # CountVectorizer по умолчанию  производит токенизацию и выкидывает слова с длиной меньшей чем два.\n",
    "X = vectorizer.fit_transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb4e9022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed5a7807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02f9cf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вики'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"кратер\"\n",
    "text_vector = vectorizer.transform([text]).toarray()[0]\n",
    "question = clf.predict([text_vector])[0]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81577767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "875c7196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([text]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "995ac9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['здравствуйте!'], dtype='<U44')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([text_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668be7a",
   "metadata": {},
   "source": [
    "### описание\n",
    "https://habr.com/ru/post/667008/\n",
    "Давно хотел сделать своего собственного Jarvis. Недавно удалась свободная минутка и я его сделал. Он умеет переписываться с Вами, а также искать ответы на Ваши вопросы в Wikipedia. Для его реализации я использовал язык Python.\n",
    "\n",
    "Для начала установим все необходимые библиотеки. Их три: pyTelegramBotAPI, scikit-learn, а также Wikipedia. Устанавливаются они просто:\n",
    "\n",
    "pip install pyTelegramBotAPI\n",
    "\n",
    "pip install Wikipedia\n",
    "\n",
    "pip install scikit-learn\n",
    "\n",
    "После установки всех библиотек приступаем к разработке. Для начала импортируем все библиотеки, установим язык для Википедии и подключим телеграмм бота\n",
    "\n",
    "import telebot, wikipedia, re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "wikipedia.set_lang(\"ru\")\n",
    "bot = telebot.TeleBot('Ваш ключ, полученный от BotFather')\n",
    "\n",
    "Теперь напишем код, для очистки всех ненужных нам знаков, которые вводит пользователь:\n",
    "\n",
    "def clean_str(r):\n",
    "\tr = r.lower()\n",
    "\tr = [c for c in r if c in alphabet]\n",
    "\treturn ''.join(r)\n",
    "\n",
    "alphabet = ' 1234567890-йцукенгшщзхъфывапролджэячсмитьбюёqwertyuiopasdfghjklzxcvbnm?%.,()!:;'\n",
    "\n",
    "Также Вам необходимо создать в папке, где находится Ваш код файл dialogues.txt, в нем мы будем создавать реплики на которые должен отвечать бот. Вот пример данного файла:\n",
    "\n",
    "привет\\здравствуйте!\n",
    "как дела\\хорошо.\n",
    "кто ты\\я Джарвис.\n",
    "\n",
    "Строка до знака \\ означает вопрос пользователя, а после ответ нашего бота. После чего напишем такой код в наш файл с ботом:\n",
    "\n",
    "def update():\n",
    "\twith open('dialogues.txt', encoding='utf-8') as f:\n",
    "\t\tcontent = f.read()\n",
    "\t\n",
    "\tblocks = content.split('\\n')\n",
    "\tdataset = []\n",
    "\t\n",
    "\tfor block in blocks:\n",
    "\t\treplicas = block.split('\\\\')[:2]\n",
    "\t\tif len(replicas) == 2:\n",
    "\t\t\tpair = [clean_str(replicas[0]), clean_str(replicas[1])]\n",
    "\t\t\tif pair[0] and pair[1]:\n",
    "\t\t\t\tdataset.append(pair)\n",
    "\t\n",
    "\tX_text = []\n",
    "\ty = []\n",
    "\t\n",
    "\tfor question, answer in dataset[:10000]:\n",
    "\t\tX_text.append(question)\n",
    "\t\ty += [answer]\n",
    "\t\n",
    "\tglobal vectorizer\n",
    "\tvectorizer = CountVectorizer()\n",
    "\tX = vectorizer.fit_transform(X_text)\n",
    "\t\n",
    "\tglobal clf\n",
    "\tclf = LogisticRegression()\n",
    "\tclf.fit(X, y)\n",
    "\n",
    "update()\n",
    "\n",
    "Этот кусок кода читает файл dialogues.txt, потом превращает реплики в так называемые вектора, с помощью которых наш бот будет искать наиболее подходящий ответ к заданному нами вопросу. Например, если Вы написали в файле dialogues.txt вопрос \"Ты знаешь Аню\", а ответ на него \"Да, конечно\", то бот будет отвечать также и на похожие вопросы, например \"Ты знаешь Васю\".\n",
    "\n",
    "Теперь напишем кусок кода, который будет генерировать ответы на основе векторов:\n",
    "\n",
    "def get_generative_replica(text):\n",
    "\ttext_vector = vectorizer.transform([text]).toarray()[0]\n",
    "\tquestion = clf.predict([text_vector])[0]\n",
    "\treturn question\n",
    "\n",
    "Этот кусок кода принимает вопрос от пользователя и возвращает ответ от бота.\n",
    "\n",
    "Теперь напишем функцию для поиска информации в Википедии:\n",
    "\n",
    "def getwiki(s):\n",
    "    try:\n",
    "        ny = wikipedia.page(s)\n",
    "        wikitext=ny.content[:1000]\n",
    "        wikimas=wikitext.split('.')\n",
    "        wikimas = wikimas[:-1]\n",
    "        wikitext2 = ''\n",
    "        for x in wikimas:\n",
    "            if not('==' in x):\n",
    "                if(len((x.strip()))>3):\n",
    "                   wikitext2=wikitext2+x+'.'\n",
    "            else:\n",
    "                break\n",
    "        wikitext2=re.sub('\\([^()]*\\)', '', wikitext2)\n",
    "        wikitext2=re.sub('\\([^()]*\\)', '', wikitext2)\n",
    "        wikitext2=re.sub('\\{[^\\{\\}]*\\}', '', wikitext2)\n",
    "        return wikitext2\n",
    "    except Exception as e:\n",
    "        return 'В Википедии нет информации об этом'\n",
    "\n",
    "Этот кусок кода получает вопрос пользователя, потом ищет ответ на него в Википедии и если ответ найден, то отдает его пользователю, а если ответ не найден, то пишет, что \"В Википедии нет информации об этом\".\n",
    "\n",
    "Теперь пишем последний кусок кода:\n",
    "\n",
    "@bot.message_handler(commands=['start'])\n",
    "def start_message(message):\n",
    "\tbot.send_message(message.chat.id,\"Здравствуйте, Сэр.\")\n",
    "\n",
    "question = \"\"\n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def get_text_messages(message):\n",
    "\tcommand = message.text.lower()\n",
    "\tif command ==\"не так\":\n",
    "\t\tbot.send_message(message.from_user.id, \"а как?\")\n",
    "\t\tbot.register_next_step_handler(message, wrong)\n",
    "\telse:\n",
    "\t\tglobal question\n",
    "\t\tquestion = command\n",
    "\t\treply = get_generative_replica(command)\n",
    "\t\tif reply==\"вики \":\n",
    "\t\t\tbot.send_message(message.from_user.id, getwiki(command))\n",
    "\t\telse:\n",
    "\t\t\tbot.send_message(message.from_user.id, reply)\n",
    "\n",
    "def wrong(message):\n",
    "\ta = f\"{question}\\{message.text.lower()} \\n\"\n",
    "\twith open('dialogues.txt', \"a\", encoding='utf-8') as f:\n",
    "\t\tf.write(a)\n",
    "\tbot.send_message(message.from_user.id, \"Готово\")\n",
    "\tupdate()\n",
    "\n",
    "В этом куске кода телеграмм бот при получении сообщения от пользователя отвечает на него и если ответ не верный, то пользователь пишет \"не так\". Если бот получает сообщение \"не так\", то он берет последний вопрос пользователя и спрашивает \"а как?\", после чего пользователь должен отправить ему правильный ответ. После этого бот обновляет свою базу данных вопросов и ответов и при следующих вопросах пользователя отвечает на них правильно. И если ответ на вопрос бот должен был взять из Википедии, то пользователь в ответ на вопрос \"а как?\", должен написать \"wiki\". Осталось в конце приписать строчку:\n",
    "\n",
    "bot.polling(none_stop=True)\n",
    "\n",
    "И можно запускать и тестировать бота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79612534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
